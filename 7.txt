import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

states = ['Noun', 'Verb']

start_prob = {'Noun': 0.9, 'Verb': 0.1}

trans_prob = {
    'Noun': {'Noun': 0.3, 'Verb': 0.7},
    'Verb': {'Noun': 0.4, 'Verb': 0.6}
}

emiss_prob = {
    'Noun': {'the': 0.9, 'dog': 0.8, 'barks': 0.1},
    'Verb': {'the': 0.05, 'dog': 0.1, 'barks': 0.85}
}

def viterbi_tagged_compact(words):
    V = [{s: start_prob[s] * emiss_prob[s].get(words[0], 1e-6) for s in states}]
    path = {s: [s] for s in states}
    
    for t in range(1, len(words)):
        V.append({})
        new_path = {}
        for cs in states:
            max_prob, best_ps = max(
                (V[t-1][ps] * trans_prob[ps][cs] * emiss_prob[cs].get(words[t], 1e-6), ps)
                for ps in states
            )
            V[t][cs] = max_prob
            new_path[cs] = path[best_ps] + [cs]
        path = new_path
    
    best_final = max(V[-1], key=V[-1].get)
    return list(zip(words, path[best_final]))

sentence = "The dog barks"
tokens = word_tokenize(sentence.lower())

tagged_words = viterbi_tagged_compact(tokens)
print(f"Sentence: {sentence}")
print(f"Tagged Words: {tagged_words}")
import multiprocessing
from collections import defaultdict
import re
from google.colab import files

uploaded = files.upload()  


filename = list(uploaded.keys())[0]


def map_function(text_chunk):
    word_counts = defaultdict(int)
    words = re.findall(r'\w+', text_chunk.lower())
    for word in words:
        word_counts[word] += 1
    return word_counts


def reduce_function(mapped_results):
    final_counts = defaultdict(int)
    for word_count in mapped_results:
        for word, count in word_count.items():
            final_counts[word] += count
    return dict(final_counts)


def mapreduce_word_count(text, num_processes=4):
    chunk_size = len(text) // num_processes
    text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    with multiprocessing.Pool(num_processes) as pool:
        mapped_results = pool.map(map_function, text_chunks)

    reduced_result = reduce_function(mapped_results)
    return reduced_result


with open(filename, 'r', encoding='utf-8') as file:
    input_text = file.read()


result = mapreduce_word_count(input_text, num_processes=4)

for word in sorted(result):
    print(f"{word}: {result[word]}")

import numpy as np
import spacy
import nltk
import re
from nltk.tokenize import word_tokenize
nltk.download('punkt')

nlp = spacy.load("en_core_web_sm")  
text = """
Natural Language Processing (NLP) is an area of artificial intelligence (AI) focused on the interaction between computers and human language. 
It enables machines to understand, interpret, and generate human language. Let's tokenize this text using spaCy, NLTK, and regular expressions!
"""

doc = nlp(text)
sentences_spacy = [sent.text for sent in doc.sents]

words_nltk = word_tokenize(text)

words_with_regex = re.findall(r'\b[A-Za-z]+\b', text)

print("Sentence Segmentation using spaCy:")
for i, sentence in enumerate(sentences_spacy, 1):
    print(f"{i}. {sentence}")

print("\nWord Tokenization using NLTK:")
print(words_nltk)

print("\nWord Tokenization using Regular Expressions:")
print(words_with_regex)
from collections import Counter
from nltk.util import bigrams
from nltk.tokenize import word_tokenize

# Sample text corpus
corpus = "I love natural language processing. I love learning."

# Tokenization
tokens = word_tokenize(corpus.lower())

# Unigram Model
unigram_counts = Counter(tokens)
total_words = sum(unigram_counts.values())

# Bigram Model
bigram_counts = Counter(bigrams(tokens))

# Calculate Unigram Probability
def unigram_prob(word):
    return unigram_counts[word] / total_words

# Calculate Bigram Probability (Add-1 Smoothing)
def bigram_prob(w1, w2):
    bigram = (w1, w2)
    return (bigram_counts[bigram] + 1) / (unigram_counts[w1] + len(unigram_counts))

# Example Calculation
print("Unigram Probability of 'love':", unigram_prob('love'))
print("Bigram Probability of 'i love':", bigram_prob('i', 'love'))